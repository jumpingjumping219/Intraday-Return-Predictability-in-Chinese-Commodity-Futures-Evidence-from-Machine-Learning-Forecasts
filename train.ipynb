{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "com_1m_bar = pd.read_parquet('/home/mw/input/com_1m_bar1102/com_30m_bar.parquet', engine = 'fastparquet')\n",
    "com_1m_bar = com_1m_bar.reset_index()\n",
    "invalid_code = ['BC', 'CJ', 'EB', 'EG', 'FU', 'I', 'JM', 'LG', 'LH', 'LU', 'NR', 'P',\n",
    "               'PF', 'PG', 'PK', 'PX', 'RR', 'SA', 'SC', 'SH', 'SP', 'SS', 'UR', 'WR', \n",
    "               'PM', 'BB', 'RI', 'JR', 'LR', 'RS', 'WH']\n",
    "com_1m_bar = com_1m_bar[~com_1m_bar['underlying_symbol'].isin(invalid_code)]\n",
    "com_1m_bar = com_1m_bar[\n",
    "    (com_1m_bar['datetime'].dt.time >= datetime.time(9, 0)) & \n",
    "    (com_1m_bar['datetime'].dt.time <= datetime.time(15, 0))\n",
    "]\n",
    "com_1m_bar = com_1m_bar[com_1m_bar.trading_date >= '2018-01-01']\n",
    "\n",
    "com_1m_bar[\"log_close\"] = np.log(com_1m_bar[\"close\"])\n",
    "com_1m_bar[\"log_ret\"]   = com_1m_bar.groupby(\"underlying_symbol\")[\"log_close\"].diff()\n",
    "pivot_ret = com_1m_bar.pivot(index=\"datetime\", columns=\"underlying_symbol\", values=\"log_ret\").iloc[1:,:]\n",
    "\n",
    "df = pivot_ret.copy()\n",
    "df.reset_index(inplace = True)\n",
    "df['date'] = df['datetime'].dt.date\n",
    "df_cleaned = df.groupby('date', group_keys=False).apply(lambda group: group.iloc[1:-1])\n",
    "df_cleaned = df_cleaned.drop(columns=['date'])\n",
    "df_cleaned.set_index('datetime', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np, pandas as pd, random, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    \"epochs\" : [50, 100],\n",
    "    \"dropout\": [0.0, 0.1, 0.2],\n",
    "    \"max_w2\" : [10., 100., 1000., np.finfo(np.float32).max],\n",
    "    \"l1\"     : [0.0, 1e-5, 1e-4],\n",
    "    \"l2\"     : [0.0, 1e-5, 1e-4],\n",
    "    \"rho\"    : [0.90, 0.95, 0.99, 0.999],\n",
    "    \"eps\"    : [1e-10, 1e-8, 1e-6, 1e-4],\n",
    "}\n",
    "\n",
    "\n",
    "def build_super_matrix(df, target, lag=3):\n",
    "    lagged = [df.shift(i).add_suffix(f\"_lag{i}\") for i in range(lag)]\n",
    "    X = pd.concat(lagged, axis=1);   y = df[target].shift(-1)\n",
    "    m = X.notna().all(1) & y.notna()\n",
    "    return X[m], y[m], X.index[m]\n",
    "\n",
    "\n",
    "def _block(sizes, drop):\n",
    "    layers=[]\n",
    "    for i in range(len(sizes)-1):\n",
    "        layers += [nn.Linear(sizes[i], sizes[i+1]),\n",
    "                   nn.BatchNorm1d(sizes[i+1]),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "        if drop>0: layers.append(nn.Dropout(drop))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "depth_cfg = {\"NN1\":[32],\n",
    "             \"NN2\":[32,16],\n",
    "             \"NN3\":[32,16,8],\n",
    "             \"NN4\":[32,16,8,4],\n",
    "             \"NN5\":[32,16,8,4,2]}\n",
    "\n",
    "def make_net(name, in_dim, drop):\n",
    "    hidden = depth_cfg[name]\n",
    "    return nn.Sequential(_block([in_dim]+hidden, drop),\n",
    "                         nn.Linear(hidden[-1],1))\n",
    "\n",
    "\n",
    "def train_one(model, X, y, X_val, y_val, cfg, batch_size=64):\n",
    "    crit = nn.SmoothL1Loss(beta=0.999)\n",
    "    opt = torch.optim.Adadelta(\n",
    "        model.parameters(),\n",
    "        rho=cfg[\"rho\"],\n",
    "        eps=cfg[\"eps\"],\n",
    "        weight_decay=cfg[\"l2\"]\n",
    "    )\n",
    "    best_state = model.state_dict()\n",
    "    best, wait = np.inf, 0\n",
    "\n",
    "    # 构建 DataLoader\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32),\n",
    "        torch.tensor(y, dtype=torch.float32)\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)  # 注意: shuffle=False\n",
    "\n",
    "    # 验证集 tensor\n",
    "    Xv = torch.tensor(X_val, dtype=torch.float32, device=DEVICE)\n",
    "    yv = torch.tensor(y_val, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb).squeeze(-1)\n",
    "            loss = crit(pred, yb)\n",
    "            if cfg[\"l1\"] > 0:\n",
    "                loss += cfg[\"l1\"] * sum(p.abs().sum() for p in model.parameters())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg[\"max_w2\"])\n",
    "            opt.step()\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vloss = crit(model(Xv).squeeze(-1), yv).item()\n",
    "\n",
    "        if vloss < best - 1e-6:\n",
    "            best, best_state, wait = vloss, model.state_dict(), 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= 10:  # early stopping\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best\n",
    "\n",
    "\n",
    "def fit_best_model(name, X_tr, y_tr, X_va, y_va,\n",
    "                   n_iter=25, n_restart=10):\n",
    "    best, best_cfg = np.inf, None\n",
    "    for _ in range(n_iter):\n",
    "        cfg = {k: random.choice(v) for k,v in param_grid.items()}\n",
    "        model = make_net(name, X_tr.shape[1], cfg[\"dropout\"]).to(DEVICE)\n",
    "        vloss = train_one(model, X_tr, y_tr, X_va, y_va, cfg)\n",
    "        if vloss < best:\n",
    "            best, best_cfg = vloss, cfg\n",
    "\n",
    "    # —— 用最佳 cfg + train+valid 合并数据，重启 n_restart 次平均 ——\n",
    "    XY = np.vstack([X_tr, X_va]);  y = np.hstack([y_tr, y_va])\n",
    "    preds = []\n",
    "    for seed in range(n_restart):\n",
    "        torch.manual_seed(seed)\n",
    "        net = make_net(name, X_tr.shape[1], best_cfg[\"dropout\"]).to(DEVICE)\n",
    "        train_one(net, XY, y, X_va, y_va, best_cfg)\n",
    "        with torch.no_grad():\n",
    "            preds.append(net(torch.tensor(XY, dtype=torch.float32, device=DEVICE))\n",
    "                         .squeeze(-1).cpu().numpy())\n",
    "    # 取 ensemble 均值模型（权重=1/n）\n",
    "    ensemble_pred = np.mean(preds, axis=0)\n",
    "    return net, ensemble_pred  # net仅作占位；真正用 scaler 反标即可\n",
    "\n",
    "\n",
    "def month_end(d):\n",
    "    y,m = d.year,d.month\n",
    "    return datetime.datetime(y+(m==12),(m%12)+1,1)-datetime.timedelta(days=1)\n",
    "\n",
    "def rolling_nn(df, target_symbol, model_name=\"NN1\", lag=3,\n",
    "               start_date=datetime.datetime(2018,1,1),\n",
    "               end_date  =datetime.datetime(2022,12,31)):\n",
    "\n",
    "    X_raw, y, idx = build_super_matrix(df, target_symbol, lag)\n",
    "    data = pd.concat([pd.DataFrame({\"y\":y.values}, index=idx), X_raw], axis=1)\n",
    "\n",
    "    preds, cur = [], start_date\n",
    "    while True:\n",
    "        tr_end  = month_end(cur + relativedelta(months=2))\n",
    "        va_end  = month_end(tr_end + relativedelta(months=1))\n",
    "        te_end  = month_end(va_end + relativedelta(months=1))\n",
    "        if te_end > end_date: break\n",
    "\n",
    "        df_tr = data[(data.index>=cur)&(data.index<=tr_end)]\n",
    "        df_va = data[(data.index>tr_end)&(data.index<=va_end)]\n",
    "        df_te = data[(data.index>va_end)&(data.index<=te_end)]\n",
    "        if min(len(df_tr),len(df_va),len(df_te)) < 100:\n",
    "            cur += relativedelta(months=1); continue\n",
    "\n",
    "        # X 标准化\n",
    "        x_scaler = StandardScaler().fit(df_tr[X_raw.columns])\n",
    "        X_tr,X_va,X_te = (x_scaler.transform(df_[X_raw.columns])\n",
    "                          for df_ in (df_tr,df_va,df_te))\n",
    "        # y 标准化 (fit on train only)\n",
    "        y_mean, y_std = df_tr.y.mean(), df_tr.y.std()\n",
    "        if y_std < 1e-12: y_std = 1.0\n",
    "        y_tr = (df_tr.y - y_mean)/y_std\n",
    "        y_va = (df_va.y - y_mean)/y_std\n",
    "\n",
    "        # 训练模型\n",
    "        model, _ = fit_best_model(model_name, X_tr, y_tr, X_va, y_va)\n",
    "\n",
    "        # 预测并反标准化\n",
    "        Xte_t = torch.tensor(X_te, dtype=torch.float32, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_pred_std = model(Xte_t).squeeze(-1).cpu().numpy()\n",
    "        y_pred = y_pred_std * y_std + y_mean\n",
    "\n",
    "        preds.append(pd.DataFrame({\n",
    "            \"datetime\": df_te.index,\n",
    "            \"y_true\":   df_te.y.values,\n",
    "            \"y_pred\":   y_pred\n",
    "        }))\n",
    "        print(f\"[{cur:%Y-%m}] test {va_end+datetime.timedelta(days=1):%Y-%m-%d}~{te_end:%Y-%m-%d}\")\n",
    "        cur += relativedelta(months=1)\n",
    "\n",
    "    return pd.concat(preds, ignore_index=True)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────\n",
    "# 6. OOS R²\n",
    "# ───────────────────────────────────────────────────────────\n",
    "def r2_os_zero(df):\n",
    "    sse_m = ((df.y_true-df.y_pred)**2).sum()\n",
    "    sse_0 = (df.y_true**2).sum()\n",
    "    return 1 - sse_m/sse_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39deb72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
